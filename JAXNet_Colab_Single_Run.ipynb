{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d7yLifhGwAtR",
      "metadata": {
        "id": "d7yLifhGwAtR"
      },
      "source": [
        "# Sweep_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ObQbAjvDvzCu",
      "metadata": {
        "id": "ObQbAjvDvzCu"
      },
      "outputs": [],
      "source": [
        "# Google Colab-friendly sweep configuration\n",
        "\n",
        "import os\n",
        "import wandb\n",
        "os.environ[\"WANDB_START_METHOD\"] = \"thread\"\n",
        "\n",
        "# Clean-Sweep-17 Hyperparameters\n",
        "\n",
        "sweep_config = {\n",
        "    'method': 'bayes',\n",
        "    'run_cap': 1,\n",
        "    'metric': {\n",
        "        'name': 'val_accuracy',\n",
        "        'goal': 'maximize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        # Architecture parameters\n",
        "        'hidden_units': {\n",
        "            'values': [\n",
        "                [512, 512, 512]\n",
        "            ]\n",
        "        },\n",
        "        'activation': {\n",
        "            'value': 'relu'\n",
        "        },\n",
        "        'weights_init': {\n",
        "            'values': ['he']\n",
        "        },\n",
        "\n",
        "        # Optimizer parameters\n",
        "        'optimizer': {\n",
        "            'values': ['adam']\n",
        "        },\n",
        "        'learning_rate': {\n",
        "            'value': 0.04573552122379638\n",
        "        },\n",
        "\n",
        "        # Regularization parameters\n",
        "        'dropout_p_value': {\n",
        "            'value': 0.3581304456936143\n",
        "        },\n",
        "        'l2_coeff': {\n",
        "            'value': 1e-8\n",
        "        },\n",
        "\n",
        "        # Training parameters\n",
        "        'batch_size': {\n",
        "            'values': [512]\n",
        "        },\n",
        "\n",
        "        # Fixed parameters\n",
        "        'num_epochs': {\n",
        "            'value': 100\n",
        "        },\n",
        "        'loss': {\n",
        "            'value': 'cross_entropy'\n",
        "        },\n",
        "        'use_grad_clipping': {\n",
        "            'value': False\n",
        "        },\n",
        "        'max_grad_norm': {\n",
        "            'value': 50.0\n",
        "        },\n",
        "        'seed': {\n",
        "            'value': 42\n",
        "        }\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uugyAM1m0LEr",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uugyAM1m0LEr",
        "outputId": "40a0a9f2-de94-49cb-eda1-e694f9ac69c9"
      },
      "outputs": [],
      "source": [
        "# To use in Google Colab:\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"JAXNet_E47B_Best_HandinTest\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c54be45c",
      "metadata": {
        "id": "c54be45c"
      },
      "source": [
        "# JAXNet.py copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd9882b6",
      "metadata": {
        "id": "bd9882b6"
      },
      "outputs": [],
      "source": [
        "#%% JAXNet Shared Functions Module\n",
        "import time\n",
        "import jax.numpy as jnp\n",
        "from jax import random, grad, jit, vmap\n",
        "from functools import partial\n",
        "import wandb\n",
        "\n",
        "class JAXNetBase:\n",
        "    \"\"\"Base class containing all shared neural network functionality\"\"\"\n",
        "\n",
        "    def __init__(self, num_features, hidden_units, num_output, weights_init='he', activation='relu', loss='cross_entropy', optimizer='sgd', l2_coeff=0.0, dropout_p=None, seed=42):\n",
        "        \"\"\"\n",
        "        Initialize neural network with configurable architecture.\n",
        "\n",
        "        Args:\n",
        "            num_features: Number of input features\n",
        "            hidden_units: List of hidden layer sizes [layer1, layer2, ...]\n",
        "            num_output: Number of output classes\n",
        "            weights_init: Weight initialization method ('he', 'xavier', 'normal')\n",
        "            activation: Activation function ('relu', 'tanh', 'sigmoid')\n",
        "            loss: Loss function ('cross_entropy', 'mse', 'mae')\n",
        "            optimizer: Optimizer type ('sgd', 'adam', 'rmsprop')\n",
        "            l2_coeff: L2 regularization coefficient (weight_decay)\n",
        "            dropout_p: List of dropout probabilities for each hidden layer (None = no dropout)\n",
        "            seed: Random seed for weight initialization\n",
        "        \"\"\"\n",
        "\n",
        "        # Build layer sizes: input → hidden layers → output\n",
        "        layer_sizes = [num_features] + hidden_units + [num_output]\n",
        "\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.activation = activation\n",
        "        self.weights_init = weights_init\n",
        "        self.loss = loss\n",
        "        self.optimizer = optimizer\n",
        "        self.l2_coeff = l2_coeff\n",
        "        self.dropout_p = dropout_p\n",
        "        self.seed = seed\n",
        "\n",
        "        # Validate dropout_p if provided\n",
        "        num_hidden = len(hidden_units)\n",
        "        if dropout_p is not None:\n",
        "            if len(dropout_p) != num_hidden:\n",
        "                raise ValueError(f\"dropout_p must have {num_hidden} values (one per hidden layer)\")\n",
        "            self.dropout_p = dropout_p\n",
        "        else:\n",
        "            self.dropout_p = [0.0] * num_hidden  # No dropout by default\n",
        "\n",
        "        # Initialize weights for each layer\n",
        "        self.W = []\n",
        "        key = random.PRNGKey(seed)\n",
        "\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            key, subkey = random.split(key)\n",
        "            input_size = layer_sizes[i]\n",
        "            output_size = layer_sizes[i + 1]\n",
        "\n",
        "            # Weight initialization\n",
        "            if weights_init == 'he':\n",
        "                # He initialization (good for ReLU)\n",
        "                w = random.normal(subkey, (input_size + 1, output_size)) * jnp.sqrt(2 / input_size)\n",
        "            elif weights_init == 'xavier':\n",
        "                # Xavier initialization (good for tanh/sigmoid)\n",
        "                w = random.normal(subkey, (input_size + 1, output_size)) * jnp.sqrt(1 / input_size)\n",
        "            elif weights_init == 'normal':\n",
        "                # Standard normal initialization\n",
        "                w = random.normal(subkey, (input_size + 1, output_size)) * 0.01\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown weights_init: {weights_init}\")\n",
        "\n",
        "            self.W.append(w)\n",
        "\n",
        "        # Initialize optimizer state\n",
        "        if optimizer == 'adam':\n",
        "            self.m = [jnp.zeros_like(w) for w in self.W]  # First moment estimates\n",
        "            self.v = [jnp.zeros_like(w) for w in self.W]  # Second moment estimates\n",
        "            self.t = 0  # Time step counter\n",
        "        elif optimizer == 'rmsprop':\n",
        "            self.v = [jnp.zeros_like(w) for w in self.W]  # Moving average of squared gradients\n",
        "\n",
        "\n",
        "    def forward(self, X, W, dropout_on=False, rng_key=None):\n",
        "        \"\"\"\n",
        "        Forward pass through the network with optional dropout\n",
        "\n",
        "        Args:\n",
        "            X: Input data\n",
        "            W: Weights\n",
        "            dropout_on: Whether to apply dropout (True during training, False during inference)\n",
        "            rng_key: JAX random key for dropout (required if dropout_on=True)\n",
        "        Returns:\n",
        "            y: Output predictions\n",
        "            h: List of hidden layer activations\n",
        "            masks: List of dropout masks (one per hidden layer)\n",
        "        \"\"\"\n",
        "        h = []\n",
        "        masks = []\n",
        "        a = X\n",
        "        num_hidden = len(W) - 1\n",
        "\n",
        "        # Loop through hidden layers\n",
        "        for l in range(num_hidden):\n",
        "            a = jnp.vstack([a, jnp.ones((1, a.shape[1]))])  # Add bias term\n",
        "            z = W[l].T @ a\n",
        "            a = self._activation_function(z)  # Use configurable activation\n",
        "\n",
        "            # Apply dropout if enabled\n",
        "            if dropout_on and self.dropout_p[l] > 0.0:\n",
        "                if rng_key is None:\n",
        "                    raise ValueError(\"rng_key must be provided when dropout_on=True\")\n",
        "                rng_key, subkey = random.split(rng_key)\n",
        "                p = self.dropout_p[l]\n",
        "                # Inverted dropout: scale active neurons to maintain expected activation\n",
        "                mask = (random.uniform(subkey, a.shape) > p).astype(float) / (1.0 - p)\n",
        "                a = a * mask\n",
        "            else:\n",
        "                mask = jnp.ones_like(a)  # No dropout: all neurons active\n",
        "\n",
        "            h.append(a)\n",
        "            masks.append(mask)\n",
        "\n",
        "        # Output layer (no dropout)\n",
        "        a = jnp.vstack([a, jnp.ones((1, a.shape[1]))])  # Add bias term\n",
        "        y_hat = W[-1].T @ a\n",
        "        y = self._softmax(y_hat)  # Output layer always uses softmax for classification\n",
        "        return y, h, masks\n",
        "\n",
        "\n",
        "    def backward(self, X, T, W, h, masks, eta, y_pred=None, use_clipping=True, max_grad_norm=25.0):\n",
        "        \"\"\"\n",
        "        Backward pass with configurable optimizers, L2 regularization, gradient clipping, and dropout.\n",
        "\n",
        "        Args:\n",
        "            X: Input data\n",
        "            T: Target labels\n",
        "            W: Weights\n",
        "            h: Hidden activations from forward pass\n",
        "            masks: Dropout masks from forward pass\n",
        "            eta: Learning rate\n",
        "            y_pred: Pre-computed predictions (optional, for efficiency)\n",
        "            use_clipping: Whether to use gradient clipping (default True)\n",
        "            max_grad_norm: Maximum gradient norm for clipping (default 25.0)\n",
        "\n",
        "        Returns:\n",
        "            W: Updated weights\n",
        "            loss: Loss value\n",
        "            grad_norms: List of gradient norms per layer\n",
        "        \"\"\"\n",
        "        m = X.shape[1]\n",
        "        grad_norms = []  # Track gradient norms per layer\n",
        "\n",
        "        if y_pred is None:  # Use pre-computed predictions if available, otherwise compute them\n",
        "            y, _, _ = self.forward(X, W, dropout_on=False)\n",
        "        else:\n",
        "            y = y_pred\n",
        "\n",
        "        # Increment Adam time step once per backward pass\n",
        "        if self.optimizer == 'adam':\n",
        "            self.t += 1\n",
        "\n",
        "        delta = self._loss_derivative(y, T)  # Use configurable loss derivative\n",
        "\n",
        "        # Backpropagate through hidden layers (in reverse)\n",
        "        for l in range(len(W) - 1, 0, -1):\n",
        "            a_prev = jnp.vstack([h[l-1], jnp.ones((1, h[l-1].shape[1]))])  # Add bias term\n",
        "            Q = a_prev @ delta.T\n",
        "\n",
        "            # Add L2 regularization to gradient (don't regularize biases - last row)\n",
        "            if self.l2_coeff > 0:\n",
        "                Q = Q.at[:-1, :].add(self.l2_coeff * W[l][:-1, :])  # Only regularize weights, not biases\n",
        "\n",
        "            # Calculate gradient norm before clipping\n",
        "            grad_norm = float(jnp.linalg.norm(Q))\n",
        "            grad_norms.append(grad_norm)\n",
        "\n",
        "            # Optional gradient clipping\n",
        "            if use_clipping:\n",
        "                Q = jnp.where(grad_norm > max_grad_norm, Q * (max_grad_norm / grad_norm), Q)\n",
        "\n",
        "            # Apply optimizer update\n",
        "            W = self._apply_optimizer_update(W, l, Q, eta, m)\n",
        "\n",
        "            # Backpropagate delta\n",
        "            delta = W[l][:-1, :] @ delta\n",
        "            delta = delta * self._activation_derivative(h[l-1])  # Use configurable activation derivative\n",
        "            delta = delta * masks[l-1]  # Apply dropout mask (only gradients through active neurons)\n",
        "\n",
        "        # First layer gradient\n",
        "        a_prev = jnp.vstack([X, jnp.ones((1, X.shape[1]))])  # Add bias term\n",
        "        Q = a_prev @ delta.T\n",
        "\n",
        "        # Add L2 regularization to first layer gradient\n",
        "        if self.l2_coeff > 0:\n",
        "            Q = Q.at[:-1, :].add(self.l2_coeff * W[0][:-1, :])  # Only regularize weights, not biases\n",
        "\n",
        "        # Calculate gradient norm for first layer before clipping\n",
        "        grad_norm = float(jnp.linalg.norm(Q))\n",
        "        grad_norms.append(grad_norm)\n",
        "\n",
        "        # Optional gradient clipping for first layer\n",
        "        if use_clipping:\n",
        "            Q = jnp.where(grad_norm > max_grad_norm, Q * (max_grad_norm / grad_norm), Q)\n",
        "\n",
        "        # Apply optimizer update to first layer\n",
        "        W = self._apply_optimizer_update(W, 0, Q, eta, m)\n",
        "        loss = self._loss_function(y, T)\n",
        "\n",
        "        # Reverse grad_norms to match layer order (0 to N)\n",
        "        grad_norms.reverse()\n",
        "\n",
        "        return W, loss, grad_norms\n",
        "\n",
        "\n",
        "    def _apply_optimizer_update(self, W, layer_idx, gradients, eta, batch_size):\n",
        "        \"\"\"Apply optimizer-specific weight updates with optional update clipping.\"\"\"\n",
        "        # Create a copy of weights list for functional update\n",
        "        W = [w for w in W]  # Shallow copy for JAX functional programming\n",
        "\n",
        "\n",
        "        if self.optimizer == 'sgd':\n",
        "            # Standard SGD update\n",
        "            W[layer_idx] = W[layer_idx] - (eta / batch_size) * gradients\n",
        "\n",
        "        elif self.optimizer == 'adam':\n",
        "            # Adam optimizer with bias correction and update clipping\n",
        "            beta1, beta2, epsilon = 0.9, 0.999, 1e-8\n",
        "            # Update biased first moment estimate\n",
        "            self.m[layer_idx] = beta1 * self.m[layer_idx] + (1 - beta1) * gradients\n",
        "            # Update biased second raw moment estimate\n",
        "            self.v[layer_idx] = beta2 * self.v[layer_idx] + (1 - beta2) * (gradients ** 2)\n",
        "            # Compute bias-corrected first moment estimate\n",
        "            m_hat = self.m[layer_idx] / (1 - beta1 ** self.t)\n",
        "            # Compute bias-corrected second raw moment estimate\n",
        "            v_hat = self.v[layer_idx] / (1 - beta2 ** self.t)\n",
        "            # Compute the raw update\n",
        "            denominator = jnp.sqrt(v_hat) + epsilon\n",
        "            update = (eta / batch_size) * m_hat / denominator\n",
        "            update = update * (batch_size / 32)\n",
        "            # Clip extreme updates to prevent instability\n",
        "            update = jnp.clip(update, -1.0, 1.0)  # Element-wise clipping\n",
        "            W[layer_idx] = W[layer_idx] - update\n",
        "\n",
        "        elif self.optimizer == 'rmsprop':\n",
        "            # RMSprop optimizer\n",
        "            decay_rate, epsilon = 0.99, 1e-8\n",
        "            # Update moving average of squared gradients\n",
        "            self.v[layer_idx] = decay_rate * self.v[layer_idx] + (1 - decay_rate) * (gradients ** 2)\n",
        "            # Apply update\n",
        "            W[layer_idx] = W[layer_idx] - (eta / batch_size) * gradients / (jnp.sqrt(self.v[layer_idx]) + epsilon)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown optimizer: {self.optimizer}\")\n",
        "\n",
        "        return W\n",
        "\n",
        "\n",
        "    def _softmax(self, y_hat):\n",
        "        \"\"\"Compute softmax probabilities\"\"\"\n",
        "        y_hat = y_hat - jnp.max(y_hat, axis=0, keepdims=True)  # prevent overflow\n",
        "        exp_scores = jnp.exp(y_hat)\n",
        "        return exp_scores / jnp.sum(exp_scores, axis=0, keepdims=True)\n",
        "\n",
        "\n",
        "    def _activation_function(self, z):\n",
        "        \"\"\"Apply activation function\"\"\"\n",
        "        if self.activation == 'relu':\n",
        "            return jnp.maximum(0, z).astype(jnp.float32)\n",
        "        elif self.activation == 'tanh':\n",
        "            return jnp.tanh(z).astype(jnp.float32)\n",
        "        elif self.activation == 'sigmoid':\n",
        "            return (1 / (1 + jnp.exp(-jnp.clip(z, -500, 500)))).astype(jnp.float32)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown activation: {self.activation}\")\n",
        "\n",
        "\n",
        "    def _activation_derivative(self, a):\n",
        "        \"\"\"Calculate derivative of activation function\"\"\"\n",
        "        if self.activation == 'relu':\n",
        "            return (a > 0).astype(jnp.float32)\n",
        "        elif self.activation == 'tanh':\n",
        "            return (1 - a**2).astype(jnp.float32)\n",
        "        elif self.activation == 'sigmoid':\n",
        "            return (a * (1 - a)).astype(jnp.float32)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown activation: {self.activation}\")\n",
        "\n",
        "\n",
        "    def _loss_function(self, y_pred, y_true):\n",
        "        \"\"\"Calculate loss based on configured loss function\"\"\"\n",
        "        epsilon = 1e-12  # Prevent log(0)\n",
        "\n",
        "        if self.loss == 'cross_entropy':\n",
        "            # Categorical Cross-Entropy Loss\n",
        "            return -jnp.sum(jnp.log(jnp.sum(y_pred * y_true, axis=0) + epsilon))\n",
        "        elif self.loss == 'mse':\n",
        "            # Mean Squared Error Loss\n",
        "            return 0.5 * jnp.sum((y_pred - y_true) ** 2)\n",
        "        elif self.loss == 'mae':\n",
        "            # Mean Absolute Error Loss\n",
        "            return jnp.sum(jnp.abs(y_pred - y_true))\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown loss function: {self.loss}\")\n",
        "\n",
        "\n",
        "    def _loss_derivative(self, y_pred, y_true):\n",
        "        \"\"\"Calculate derivative of loss function for backpropagation\"\"\"\n",
        "        if self.loss == 'cross_entropy':\n",
        "            # For cross-entropy with softmax: derivative is simply (y_pred - y_true)\n",
        "            return y_pred - y_true\n",
        "        elif self.loss == 'mse':\n",
        "            # MSE derivative: (y_pred - y_true)\n",
        "            return y_pred - y_true\n",
        "        elif self.loss == 'mae':\n",
        "            # MAE derivative: sign(y_pred - y_true)\n",
        "            return jnp.sign(y_pred - y_true)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown loss function: {self.loss}\")\n",
        "\n",
        "\n",
        "# Shared utility functions\n",
        "def calculate_accuracy(net, X, T, W):\n",
        "    \"\"\"Calculate accuracy percentage (always with dropout OFF)\"\"\"\n",
        "    y, _, _ = net.forward(X, W, dropout_on=False)\n",
        "    predictions = jnp.argmax(y, axis=0)\n",
        "    true_labels = jnp.argmax(T, axis=0)\n",
        "    return jnp.mean(predictions == true_labels) * 100\n",
        "\n",
        "\n",
        "def train(net, X, T, W, epochs, eta, batchsize=32, X_val=None, T_val=None, use_clipping=True, max_grad_norm=25.0, use_wandb=False, wandb_project=None, wandb_config=None, wandb_mode=\"online\"):\n",
        "    \"\"\"\n",
        "    Training loop for neural network with mandatory validation.\n",
        "\n",
        "    Args:\n",
        "        net: Neural network instance\n",
        "        X, T: Training data and labels\n",
        "        W: Initial weights\n",
        "        epochs: Number of training epochs\n",
        "        eta: Learning rate\n",
        "        batchsize: Mini-batch size\n",
        "        X_val, T_val: Validation data and labels (required)\n",
        "        use_clipping: Whether to use gradient clipping\n",
        "        max_grad_norm: Maximum gradient norm for clipping\n",
        "        use_wandb: Whether to use Weights & Biases logging\n",
        "        wandb_project: W&B project name\n",
        "        wandb_config: Dictionary of hyperparameters to log to W&B\n",
        "        wandb_mode: W&B mode - \"online\", \"offline\", or \"disabled\"\n",
        "    \"\"\"\n",
        "    losses = []\n",
        "    val_losses = []  # Track validation loss\n",
        "    train_accuracies = []  # Track training accuracy\n",
        "    val_accuracies = []  # Track validation accuracy\n",
        "    epoch_times = []  # Track computation time per epoch\n",
        "\n",
        "    # Initialize W&B if enabled\n",
        "    if use_wandb and wandb_project:\n",
        "        wandb.init(project=wandb_project, config=wandb_config, mode=wandb_mode)\n",
        "\n",
        "    # Print header for nicely formatted table\n",
        "    print(\"-\" * 90)\n",
        "    print(f\"{'Epoch':<10} {'Train Acc':<12} {'Val Acc':<12} {'Gain':<10} {'Time':<10} {'ETA'}\")\n",
        "    print(\"-\" * 90)\n",
        "\n",
        "    start_total = time.time()\n",
        "\n",
        "    m = X.shape[1]  # Training set size\n",
        "    m_val = X_val.shape[1]  # Validation set size\n",
        "    key = random.PRNGKey(net.seed)  # For batch shuffling and dropout - uses network's seed\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_start = time.time()  # Start timing this epoch\n",
        "\n",
        "        # Track gradient norms for this epoch\n",
        "        epoch_grad_norms = []\n",
        "\n",
        "        key, subkey = random.split(key)\n",
        "        order = random.permutation(subkey, m)\n",
        "\n",
        "        for i in range(0, m, batchsize):\n",
        "            batch = order[i:i+batchsize]\n",
        "            X_batch = X[:, batch]\n",
        "            T_batch = T[:, batch]\n",
        "\n",
        "            # Forward pass with dropout enabled during training\n",
        "            key, dropout_key = random.split(key)\n",
        "            y_batch, h, masks = net.forward(X_batch, W, dropout_on=True, rng_key=dropout_key)\n",
        "\n",
        "            # Backward pass with dropout masks (this updates weights with dropout active)\n",
        "            W, loss, grad_norms = net.backward(X_batch, T_batch, W, h, masks, eta, y_batch, use_clipping, max_grad_norm)\n",
        "            epoch_grad_norms.append(grad_norms)\n",
        "\n",
        "        # Calculate training accuracy and loss (with dropout OFF for fair comparison)\n",
        "        train_accuracy = float(calculate_accuracy(net, X, T, W))\n",
        "        train_accuracies.append(train_accuracy)\n",
        "        # Calculate training loss with dropout OFF\n",
        "        y_train_pred, _, _ = net.forward(X, W, dropout_on=False)\n",
        "        train_loss = net._loss_function(y_train_pred, T)\n",
        "        # Normalize training loss by dataset size (average loss per sample)\n",
        "        losses.append(float(train_loss / m))\n",
        "\n",
        "        # Calculate validation accuracy and loss\n",
        "        val_accuracy = float(calculate_accuracy(net, X_val, T_val, W))\n",
        "        val_accuracies.append(val_accuracy)\n",
        "        # Calculate validation loss\n",
        "        y_val_pred, _, _ = net.forward(X_val, W, dropout_on=False)\n",
        "        val_loss = net._loss_function(y_val_pred, T_val)\n",
        "        # Normalize validation loss by dataset size (average loss per sample)\n",
        "        val_losses.append(float(val_loss / m_val))\n",
        "\n",
        "        # Calculate average gradient norms across all batches in this epoch\n",
        "        import numpy as np  # Use numpy for averaging lists\n",
        "        avg_grad_norms = np.mean(epoch_grad_norms, axis=0)  # Average over batches\n",
        "        total_grad_norm = float(jnp.linalg.norm(jnp.array(avg_grad_norms)))  # Total gradient norm\n",
        "\n",
        "        # Calculate gain compared to last epoch\n",
        "        if epoch > 0:\n",
        "            gain = train_accuracy - train_accuracies[-2]  # Current - previous\n",
        "            if gain > 0:\n",
        "                gain_str = f\"+{gain:.2f}%\"\n",
        "            elif gain < 0:\n",
        "                gain_str = f\"{gain:.2f}%\"  # Already has negative sign\n",
        "            else:\n",
        "                gain_str = \" 0.00%\"\n",
        "        else:\n",
        "            gain_str = \"baseline\"\n",
        "\n",
        "        # Calculate epoch time\n",
        "        epoch_time = time.time() - epoch_start\n",
        "        epoch_times.append(epoch_time)\n",
        "\n",
        "        # Calculate ETA (estimated time remaining)\n",
        "        if epoch > 0:\n",
        "            avg_time_per_epoch = sum(epoch_times) / len(epoch_times)  # Use pure Python for averaging\n",
        "            remaining_epochs = epochs - (epoch + 1)\n",
        "            eta_seconds = avg_time_per_epoch * remaining_epochs\n",
        "            if eta_seconds > 60:\n",
        "                eta_str = f\"{int(eta_seconds//60)}min {int(eta_seconds%60)}sec\"\n",
        "            else:\n",
        "                eta_str = f\"{int(eta_seconds)}sec\"\n",
        "        else:\n",
        "            eta_str = \"calculating...\"\n",
        "\n",
        "        # Format epoch info for output\n",
        "        epoch_str = f\"{epoch+1}/{epochs}\"\n",
        "        accuracy_str = f\"{train_accuracy:.2f}%\"\n",
        "        val_acc_str = f\"{val_accuracy:.2f}%\"\n",
        "        time_str = f\"{epoch_time:.2f}sec\"\n",
        "\n",
        "        # Show progress\n",
        "        print(f\"{epoch_str:<10} {accuracy_str:<12} {val_acc_str:<12} {gain_str:<10} {time_str:<10} {eta_str}\")\n",
        "\n",
        "        # Log to W&B if enabled\n",
        "        if use_wandb and (wandb_project or wandb.run is not None):\n",
        "            # Prepare log dictionary\n",
        "            log_dict = {\n",
        "                \"epoch\": epoch + 1,\n",
        "                \"train_loss\": float(losses[-1]),  # Use normalized loss\n",
        "                \"train_accuracy\": float(train_accuracy),\n",
        "                \"val_accuracy\": float(val_accuracy),\n",
        "                \"val_loss\": float(val_losses[-1]),  # Use normalized loss\n",
        "                \"epoch_time\": float(epoch_time),\n",
        "                \"grad_norm_total\": float(total_grad_norm)  # Total gradient norm\n",
        "            }\n",
        "\n",
        "            # Log per-layer gradient norms\n",
        "            for layer_idx, grad_norm in enumerate(avg_grad_norms):\n",
        "                log_dict[f\"grad_norm_layer_{layer_idx}\"] = float(grad_norm)\n",
        "\n",
        "            # Log parameter histograms every 1/10th of total epochs\n",
        "            histogram_interval = max(1, epochs // 10)\n",
        "            if (epoch + 1) % histogram_interval == 0 or epoch == 0:\n",
        "                for layer_idx, w in enumerate(W):\n",
        "                    # Separate weights and biases (bias is the last row)\n",
        "                    weights = np.array(w[:-1, :])  # All rows except last\n",
        "                    biases = np.array(w[-1, :])    # Last row\n",
        "                    log_dict[f\"weights_layer_{layer_idx}\"] = wandb.Histogram(weights.flatten())\n",
        "                    log_dict[f\"biases_layer_{layer_idx}\"] = wandb.Histogram(biases.flatten())\n",
        "\n",
        "            wandb.log(log_dict)\n",
        "\n",
        "    total_time = time.time() - start_total\n",
        "    avg_epoch_time = sum(epoch_times) / len(epoch_times)\n",
        "\n",
        "    print(\"-\" * 70)\n",
        "    print(f\"Total training time: {total_time:.1f}sec\")\n",
        "    print(f\"Average per epoch: {avg_epoch_time:.2f}sec\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    # Don't finish W&B here - let evaluate_model do it after logging test metrics\n",
        "\n",
        "    return W, losses, train_accuracies, val_accuracies, val_losses\n",
        "\n",
        "def evaluate_model(net, X_test, T_test, y_test, W, train_accuracies, use_wandb=False):\n",
        "    \"\"\"\n",
        "    Evaluate model performance and print results\n",
        "\n",
        "    Args:\n",
        "        net: Neural network instance\n",
        "        X_test, T_test: Test data and labels\n",
        "        y_test: Test labels (not one-hot encoded)\n",
        "        W: Trained weights\n",
        "        train_accuracies: List of training accuracies from training\n",
        "        use_wandb: Whether to log test metrics to W&B\n",
        "    \"\"\"\n",
        "    # Make predictions and calculate accuracy (dropout OFF for evaluation)\n",
        "    y_test_pred, _, _ = net.forward(X_test.T, W, dropout_on=False)\n",
        "    y_pred = jnp.argmax(y_test_pred, axis=0)\n",
        "    test_accuracy = float(jnp.mean(y_pred == y_test))  # Convert to Python float\n",
        "\n",
        "    # Calculate test loss using the configurable loss function\n",
        "    test_loss = float(net._loss_function(y_test_pred, T_test.T) / X_test.shape[0])  # Average per sample\n",
        "\n",
        "    print(f\"\\n================== Final Results ==================\")\n",
        "    print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "    print(f\"Test Loss (avg per sample): {test_loss:.4f}\")\n",
        "    print(f\"Training Accuracy Improvement: {(train_accuracies[-1] - train_accuracies[0]):.1f}% points\")\n",
        "    print(f\"Final Training Accuracy: {train_accuracies[-1]:.2f}%\")\n",
        "\n",
        "    # Log test metrics to W&B if enabled and run is still active\n",
        "    if use_wandb and wandb.run is not None:\n",
        "        wandb.log({\n",
        "            \"test_accuracy\": float(test_accuracy * 100),\n",
        "            \"test_loss\": float(test_loss)\n",
        "        })\n",
        "        wandb.finish(quiet=False)  # Finish the W&B run after logging test metrics (quiet=False shows summary)\n",
        "\n",
        "    return y_pred, test_accuracy, test_loss\n",
        "\n",
        "def plot_training_results(losses, train_accuracies, val_accuracies, val_losses, test_accuracy=None, figsize=(15, 5), save_path=None):\n",
        "    \"\"\"\n",
        "    Plot training curves including loss and accuracy over epochs.\n",
        "\n",
        "    Args:\n",
        "        losses: List of training loss values per epoch\n",
        "        train_accuracies: List of training accuracy values per epoch\n",
        "        val_accuracies: List of validation accuracy values per epoch (required)\n",
        "        val_losses: List of validation loss values per epoch (required)\n",
        "        test_accuracy: Final test accuracy (optional, shown as horizontal line)\n",
        "        figsize: Figure size (width, height)\n",
        "        save_path: Optional path to save the figure (e.g., 'training_curves.png')\n",
        "\n",
        "    Returns:\n",
        "        fig: Matplotlib figure object\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "\n",
        "    epochs = range(1, len(losses) + 1)\n",
        "\n",
        "    # Create figure with 3 subplots\n",
        "    fig, axes = plt.subplots(1, 3, figsize=figsize)\n",
        "\n",
        "    # Plot 1: Training and Validation Loss\n",
        "    axes[0].plot(epochs, losses, 'b-', linewidth=2, label='Training Loss')\n",
        "    axes[0].plot(epochs, val_losses, 'orange', linestyle='--', linewidth=2, label='Validation Loss')\n",
        "    axes[0].set_xlabel('Epoch', fontsize=11)\n",
        "    axes[0].set_ylabel('Loss', fontsize=11)\n",
        "    axes[0].set_title('Loss Over Time', fontsize=12, fontweight='bold')\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    axes[0].legend()\n",
        "\n",
        "    # Plot 2: Training and Validation Accuracy\n",
        "    axes[1].plot(epochs, train_accuracies, 'g-', linewidth=2, label='Training Accuracy')\n",
        "    axes[1].plot(epochs, val_accuracies, 'orange', linestyle='--', linewidth=2, label='Validation Accuracy')\n",
        "    if test_accuracy is not None:\n",
        "        axes[1].axhline(y=test_accuracy * 100, color='r', linestyle='--', linewidth=2,\n",
        "                       label=f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "    axes[1].set_xlabel('Epoch', fontsize=11)\n",
        "    axes[1].set_ylabel('Accuracy (%)', fontsize=11)\n",
        "    axes[1].set_title('Accuracy Over Time', fontsize=12, fontweight='bold')\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    axes[1].legend()\n",
        "\n",
        "    # Plot 3: Accuracy Improvement (delta between consecutive epochs for train and val)\n",
        "    train_deltas = [0] + [train_accuracies[i] - train_accuracies[i-1]\n",
        "                          for i in range(1, len(train_accuracies))]\n",
        "    val_deltas = [0] + [val_accuracies[i] - val_accuracies[i-1]\n",
        "                       for i in range(1, len(val_accuracies))]\n",
        "\n",
        "    # Bar width for side-by-side bars\n",
        "    width = 0.5\n",
        "    x = np.array(list(epochs))\n",
        "\n",
        "    # Create bars\n",
        "    train_colors = ['g' if d >= 0 else 'r' for d in train_deltas]\n",
        "    val_colors = ['orange' if d >= 0 else 'darkred' for d in val_deltas]\n",
        "\n",
        "    axes[2].bar(x - width/2, train_deltas, width, color=train_colors, alpha=0.6, label='Train')\n",
        "    axes[2].bar(x + width/2, val_deltas, width, color=val_colors, alpha=0.6, label='Val')\n",
        "    axes[2].legend()\n",
        "    axes[2].axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
        "    axes[2].set_xlabel('Epoch', fontsize=11)\n",
        "    axes[2].set_ylabel('Accuracy Change (%)', fontsize=11)\n",
        "    axes[2].set_title('Per-Epoch Accuracy Gain/Loss', fontsize=12, fontweight='bold')\n",
        "    axes[2].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "        print(f\"Plot saved to: {save_path}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    return fig\n",
        "\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, class_names=None, normalize=False, figsize=(8, 6), save_path=None):\n",
        "    \"\"\"\n",
        "    Plot confusion matrix for model predictions.\n",
        "\n",
        "    Args:\n",
        "        y_true: True labels (1D array of class indices)\n",
        "        y_pred: Predicted labels (1D array of class indices)\n",
        "        class_names: List of class names (optional, uses indices if None)\n",
        "        normalize: Whether to normalize by row (True) or show raw counts (False)\n",
        "        figsize: Figure size (width, height)\n",
        "        save_path: Optional path to save the figure (e.g., 'confusion_matrix.png')\n",
        "\n",
        "    Returns:\n",
        "        fig: Matplotlib figure object\n",
        "        cm: Confusion matrix array\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    import numpy as np\n",
        "\n",
        "    # Convert JAX arrays to numpy if needed\n",
        "    if hasattr(y_true, 'device'):  # Check if it's a JAX array\n",
        "        y_true = np.array(y_true)\n",
        "    if hasattr(y_pred, 'device'):  # Check if it's a JAX array\n",
        "        y_pred = np.array(y_pred)\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Normalize if requested\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        fmt = '.2f'\n",
        "        title = 'Normalized Confusion Matrix'\n",
        "    else:\n",
        "        fmt = 'd'\n",
        "        title = 'Confusion Matrix'\n",
        "\n",
        "    # Set up class names\n",
        "    if class_names is None:\n",
        "        class_names = [str(i) for i in range(len(cm))]\n",
        "\n",
        "    # Create figure\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "\n",
        "    # Set ticks and labels\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           xticklabels=class_names,\n",
        "           yticklabels=class_names,\n",
        "           title=title,\n",
        "           ylabel='True Label',\n",
        "           xlabel='Predicted Label')\n",
        "\n",
        "    # Rotate x labels for readability\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
        "\n",
        "    # Add text annotations\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], fmt),\n",
        "                   ha=\"center\", va=\"center\",\n",
        "                   color=\"white\" if cm[i, j] > thresh else \"black\",\n",
        "                   fontsize=8)\n",
        "\n",
        "    fig.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=850, bbox_inches='tight')\n",
        "        print(f\"Confusion matrix saved to: {save_path}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    return fig, cm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b597742",
      "metadata": {
        "id": "8b597742"
      },
      "source": [
        "# JAXNet_E47B_Sweep.py copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e7879db",
      "metadata": {
        "id": "5e7879db"
      },
      "outputs": [],
      "source": [
        "#%%########### JAXNet_E47B_Sweep: EMNIST Balanced Hyperparameter Sweep ##############\n",
        "\"\"\"\n",
        "EMNIST Balanced hyperparameter sweep script for WandB using JAX.\n",
        "Works with sweep config to automatically test different configurations.\n",
        "\n",
        "Usage:\n",
        "    1. Run: wandb sweep --entity <team> --project <project> sweep_config.yaml\n",
        "    2. Copy the sweep ID\n",
        "    3. Run: wandb agent <sweep_id>\n",
        "\n",
        "This script will be called automatically by WandB for each sweep run.\n",
        "\"\"\"\n",
        "\n",
        "import warnings\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import wandb\n",
        "import tensorflow_datasets as tfds\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "\n",
        "\n",
        "def preprocess_data(ds):\n",
        "    \"\"\"Convert a TFDS dataset split to JAX arrays.\"\"\"\n",
        "    images, labels = [], []\n",
        "    for image, label in ds:\n",
        "        images.append(image.numpy())\n",
        "        labels.append(label.numpy())\n",
        "    return jnp.array(images), jnp.array(labels)\n",
        "\n",
        "\n",
        "def train_sweep():\n",
        "    \"\"\"\n",
        "    Main training function called by WandB sweep agent.\n",
        "    Gets hyperparameters from wandb.config and runs one training experiment.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "    # Initialize WandB run (sweep agent handles this automatically)\n",
        "    run = wandb.init()\n",
        "\n",
        "    # Get hyperparameters from sweep config\n",
        "    config = wandb.config\n",
        "\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"Starting EMNIST Balanced sweep run: {run.name}\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "\n",
        "    #%%######################### 1. Dataset Configuration ####################\n",
        "\n",
        "    num_features = 28 * 28     # EMNIST: 28x28 pixels\n",
        "    num_classes = 47           # EMNIST Balanced: 47 classes (merged digits and letters)\n",
        "\n",
        "\n",
        "    #%%######################### 2. Load EMNIST Data #########################\n",
        "\n",
        "    print(\"\\nLoading EMNIST Balanced dataset...\")\n",
        "    ds_train, ds_test = tfds.load('emnist/balanced', split=['train', 'test'], as_supervised=True)\n",
        "\n",
        "    print(\"Converting to JAX arrays...\")\n",
        "    X_train_full, y_train_full = preprocess_data(ds_train)\n",
        "    X_test, y_test = preprocess_data(ds_test)\n",
        "\n",
        "    # Split training into train/validation (90/10)\n",
        "    # Convert to numpy for train_test_split, then back to JAX\n",
        "    import numpy as np\n",
        "    X_train_np, X_val_np, y_train_np, y_val_np = train_test_split(\n",
        "        np.array(X_train_full), np.array(y_train_full),\n",
        "        test_size=0.1, random_state=42, stratify=np.array(y_train_full)\n",
        "    )\n",
        "\n",
        "    # Reshape and normalize inputs\n",
        "    X_train = jnp.array(X_train_np.reshape(-1, 28*28) / 255.0)\n",
        "    X_val = jnp.array(X_val_np.reshape(-1, 28*28) / 255.0)\n",
        "    X_test = X_test.reshape(-1, 28*28) / 255.0\n",
        "\n",
        "    # One-hot encode labels (0-46 for 47 classes)\n",
        "    T_train = to_categorical(y_train_np, num_classes=num_classes)\n",
        "    T_val = to_categorical(y_val_np, num_classes=num_classes)\n",
        "    T_test = to_categorical(np.array(y_test), num_classes=num_classes)\n",
        "\n",
        "    # Convert to JAX arrays\n",
        "    T_train = jnp.array(T_train)\n",
        "    T_val = jnp.array(T_val)\n",
        "    T_test = jnp.array(T_test)\n",
        "    y_test = np.array(y_test)  # Keep as numpy for evaluation\n",
        "\n",
        "    print(f\"Training: {X_train.shape[0]:,} | Validation: {X_val.shape[0]:,} | Test: {X_test.shape[0]:,}\")\n",
        "\n",
        "\n",
        "    #%%################### 3. Extract Sweep Hyperparameters ##################\n",
        "\n",
        "    # Architecture\n",
        "    hidden_units = config.hidden_units\n",
        "    activation = config.activation\n",
        "    weights_init = config.weights_init\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = config.optimizer\n",
        "    learning_rate = config.learning_rate\n",
        "\n",
        "    # Regularization\n",
        "    dropout_p_value = config.dropout_p_value\n",
        "    # Create dropout list matching number of hidden layers\n",
        "    num_hidden_layers = len(hidden_units)\n",
        "    dropout_p = [dropout_p_value] * num_hidden_layers\n",
        "    l2_coeff = config.l2_coeff\n",
        "\n",
        "    # Training\n",
        "    batch_size = config.batch_size\n",
        "    num_epochs = config.num_epochs\n",
        "    loss = config.loss\n",
        "    use_grad_clipping = config.use_grad_clipping\n",
        "    max_grad_norm = config.max_grad_norm\n",
        "\n",
        "    # Seed\n",
        "    seed = config.seed\n",
        "\n",
        "\n",
        "    #%%################ 4. Print Configuration ####################\n",
        "\n",
        "    print(f\"\\nConfiguration for this run:\")\n",
        "    print(f\"  Architecture: {hidden_units} | {activation} | {weights_init}\")\n",
        "    print(f\"  Optimizer: {optimizer} | LR: {learning_rate}\")\n",
        "    print(f\"  Regularization: dropout={dropout_p_value} | L2={l2_coeff}\")\n",
        "    print(f\"  Training: batch={batch_size} | epochs={num_epochs}\")\n",
        "    print(f\"  Seed: {seed}\")\n",
        "    print()\n",
        "\n",
        "\n",
        "    #%%################### 5. Initialize Neural Network ######################\n",
        "\n",
        "    # Create EMNIST Balanced-specific network class\n",
        "    class JAXNet_E47B(JAXNetBase):\n",
        "        \"\"\"EMNIST Balanced-specific neural network using shared base functionality\"\"\"\n",
        "        pass\n",
        "\n",
        "    # Initialize network\n",
        "    net = JAXNet_E47B(\n",
        "        num_features, hidden_units, num_classes,\n",
        "        weights_init, activation, loss, optimizer, l2_coeff, dropout_p,\n",
        "        seed=seed\n",
        "    )\n",
        "\n",
        "\n",
        "    #%%########################### 6. Training Loop ##########################\n",
        "\n",
        "    # Train the model (WandB is already initialized by sweep agent)\n",
        "    # Pass None for wandb_project to prevent double initialization\n",
        "    net.W, losses, train_accuracies, val_accuracies, val_losses = train(\n",
        "        net, X_train.T, T_train.T, net.W,\n",
        "        num_epochs, learning_rate, batch_size,\n",
        "        X_val=X_val.T, T_val=T_val.T,\n",
        "        use_clipping=use_grad_clipping,\n",
        "        max_grad_norm=max_grad_norm,\n",
        "        use_wandb=True,  # Always True for sweeps\n",
        "        wandb_project=None,  # None = don't reinitialize, sweep agent already did it\n",
        "        wandb_config=None,  # None = don't reinitialize, sweep agent already did it\n",
        "        wandb_mode=\"online\"  # Sweeps must be online\n",
        "    )\n",
        "\n",
        "\n",
        "    #%%########################## 7. Evaluate Model ##########################\n",
        "\n",
        "    # Evaluate and display results\n",
        "    y_pred, test_accuracy, test_loss = evaluate_model(\n",
        "        net, X_test, T_test, y_test, net.W, train_accuracies, use_wandb=True\n",
        "    )\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Sweep run {run.name} complete!\")\n",
        "    print(f\"Final validation accuracy: {val_accuracies[-1]:.2f}%\")\n",
        "    print(f\"Test accuracy: {test_accuracy * 100:.2f}%\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    # Note: WandB run is finished by evaluate_model()\n",
        "    \n",
        "    # Return predictions and labels for confusion matrix\n",
        "    return y_pred, y_test, test_accuracy\n",
        "\n",
        "\n",
        "#%%######################### Main Execution ##################################\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # This function will be called by WandB sweep agent\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MmopGlIn0SIy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MmopGlIn0SIy",
        "outputId": "f2563907-fcaf-497a-afdb-2c58f0f7762e"
      },
      "outputs": [],
      "source": [
        "# Run the training and capture results for confusion matrix\n",
        "# Note: Since wandb.agent doesn't return values, we'll use a global variable\n",
        "results = {}\n",
        "\n",
        "def train_sweep_wrapper():\n",
        "    y_pred, y_test, test_accuracy = train_sweep()\n",
        "    results['y_pred'] = y_pred\n",
        "    results['y_test'] = y_test\n",
        "    results['test_accuracy'] = test_accuracy\n",
        "    return y_pred, y_test, test_accuracy\n",
        "\n",
        "wandb.agent(sweep_id, function=train_sweep_wrapper, count=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77425aa0",
      "metadata": {},
      "source": [
        "# Plot Confusion Matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e98edba",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get predictions and labels from training results\n",
        "print(\"Preparing confusion matrix data...\")\n",
        "\n",
        "y_pred = results['y_pred']\n",
        "y_test = results['y_test']\n",
        "test_accuracy = results['test_accuracy']\n",
        "\n",
        "print(f\"Predictions shape: {y_pred.shape}\")\n",
        "print(f\"True labels shape: {y_test.shape}\")\n",
        "print(f\"Test accuracy: {test_accuracy * 100:.2f}%\")\n",
        "print(\"\\nReady to plot confusion matrix!\")\n",
        "\n",
        "# EMNIST Balanced class names (47 classes)\n",
        "class_names = [\n",
        "    '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
        "    'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J',\n",
        "    'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T',\n",
        "    'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'd', 'e',\n",
        "    'f', 'g', 'h', 'n', 'q', 'r', 't'\n",
        "]\n",
        "\n",
        "# Plot confusion matrix (raw counts)\n",
        "print(\"Generating confusion matrix...\")\n",
        "fig_cm, cm = plot_confusion_matrix(\n",
        "    y_test, y_pred,\n",
        "    class_names=class_names,\n",
        "    normalize=False,\n",
        "    figsize=(12, 10),\n",
        "    save_path='confusion_matrix_best_model.png'\n",
        ")\n",
        "\n",
        "print(\"✅ Confusion matrix saved as 'confusion_matrix_best_model.png'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecd5ae97",
      "metadata": {},
      "source": [
        "# Analysis: Most Confused Classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6418f33d",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Top 10 Most Confused Class Pairs:\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Get off-diagonal elements (misclassifications)\n",
        "confusion_pairs = []\n",
        "for i in range(len(cm)):\n",
        "    for j in range(len(cm)):\n",
        "        if i != j:  # Exclude diagonal (correct predictions)\n",
        "            confusion_pairs.append((cm[i, j], class_names[i], class_names[j]))\n",
        "\n",
        "# Sort by confusion count\n",
        "confusion_pairs.sort(reverse=True)\n",
        "\n",
        "# Display top 10\n",
        "for idx, (count, true_label, pred_label) in enumerate(confusion_pairs[:10], 1):\n",
        "    percentage = (count / cm.sum()) * 100\n",
        "    print(f\"{idx:2d}. True: '{true_label}' → Predicted: '{pred_label}' | Count: {count:4d} ({percentage:.2f}%)\")\n",
        "\n",
        "print(\"=\" * 70)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
